## 为啥需要fastText分类 
#### 论文地址
    
    https://arxiv.org/pdf/1607.01759v3.pdf
#### 本文数据来源
    https://blog.csdn.net/weixin_36604953/article/details/78195462
    https://blog.csdn.net/weixin_36604953/article/details/78324834
    
    2013年，Google的大牛Tomas Mikolov开源了word2vec算法，轰动一时（当然，托马斯大牛现在于FaceBook就职，16年中下旬开源了fastText算法，又在业界引起了轩然大波，没办法，速度太快了）
业内最新潮的文本分类算法，fastText与word2vec的提出者之所以会想到用fastText取代CNN等深度学习模型，目的就是提升运算速度。
 
    对于文本来说，与图像不同，并不需要过多的隐藏层网络，比如一句话表达了某种积极的意义，当你听了三遍的时候，已经判断出了极性，而不需要再听了。对于图像， 为了提取到细节上的特征，你需要进行多次的训练，否则是无法做到的。你不可能看了三遍就能知道图像的每个细节，当出现了某个类似的图像的时候，你是无法判断真假的。
    所以用简单的网络对自然语言进行学习，可以死快速高质量的得到结果。
    这就是fastText的目前所在。
## faseText模型架构
### fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。
继续沿用worc2vec词向量的结构，与CBOW架构类似，不同的是CBOW预测
得到的基于上下文的中间单词的向量。
fastText输入的是一个词的序列，输入是这个词序列属于不同类的的概率。
     
     在序列中词和词组成特征向量，特征向量通过线性变换映射到中间层，再由中间层映射到标签。在预测便签时使用非线性激活函数，在中间层不适用非线性激励函数。
     文本特征一般由词袋模型获取，考虑到词序问题，fastText使用了n-gram模型获取局部的词序。
## 分层softmax
 
Facebook声称fastText比其他学习方法要快得多，能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”，特别是与深度模型对比，fastText能将训练时间由数天缩短到几秒钟。这样的速度取决于什么呢？模型架构上并没有什么本质革新，接下来就带你“上高速”~
### softmax函数

     归一化函数，将k维的向量映射到0-1之间的常数。根据常数的大小进行分类任务。
#### 分层softmax

    分层softmax的目的是降低soft层的计算复杂度。用层级关系代替了扁平化的softmax层。
    可以把原来的softmax看做是深度为1的树，词表中每个词语表示一个叶子及诶单，若谷吧softmax改为二叉树架构，每个word表示叶子节点，那么只需沿着桐乡该词语的叶子节点的搜索路径，而不需要考虑其他的节点。
重点：为啥快？

     在标准的softmax回归中，要计算y=j的概率，需要将k维数据做归一化
     分层的softmax则只需要归一化某条路径上的所有节点的概率值。
### N-gram
考虑了词序问题。
### 实践
    
    安装使用pip或者conda install fasttext
    参考网址 https://blog.csdn.net/meyh0x5vDTk48P2/article/details/79055553
####
	
     
     
     

    
    